{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasad/miniconda3/envs/gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import langchain\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A4500. Max memory: 19.696 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "received_prompt = \"\"\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Innovent/trained_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Below is a prompt that describes any question a user has or a problem being faced by the user. Write a response that appropriately helps the user answer his question or give the steps to troubleshoot his problem.\n",
    "\n",
    "### userPrompt:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722337611.881950 1218290 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    verbose=True,\n",
    "    temperature=0,\n",
    "    google_api_key=\"AIzaSyBpVRT86uMPCk7tKX_q-x3Ula8U8ucaiMA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(response):\n",
    "  for line in response.split(\"\\n\"):\n",
    "    if line.strip().isdigit():\n",
    "      return int(line.strip())\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 500,\n",
    ")\n",
    "def load_pdf(path):\n",
    "  loader = PyPDFLoader(path)\n",
    "  document = loader.load()\n",
    "  text = text_splitter.split_documents(document)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(original_query):\n",
    "  prompt = ChatPromptTemplate(input_variables=['original_query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant that generates multiple search queries based on a single input query which is related to troubleshooting a car.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (4 queries):'))])\n",
    "  generate_queries = (\n",
    "    prompt | gemini | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "  )\n",
    "  return generate_queries.invoke(original_query)\n",
    "\n",
    "def vector_search(query, all_documents):\n",
    "    documents = list(all_documents.values())\n",
    "    doc_names = list(all_documents.keys())\n",
    "\n",
    "    # Combine the query and documents\n",
    "    combined = [query] + documents\n",
    "\n",
    "    # Vectorize the combined texts\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined)\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "\n",
    "    # Get similarity scores and sort them\n",
    "    scores = cosine_sim.flatten()\n",
    "    score_dict = {doc_names[i]: round(scores[i], 2) for i in range(len(scores))}\n",
    "    sorted_scores = {doc: score for doc, score in sorted(score_dict.items(), key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "    return sorted_scores\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion_docs(search_results_dict, k=60):\n",
    "    fused_scores = {}\n",
    "    print(\"Initial individual search result ranks:\")\n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        print(f\"For query '{query}': {doc_scores}\")\n",
    "\n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if doc not in fused_scores:\n",
    "                fused_scores[doc] = 0\n",
    "            previous_score = fused_scores[doc]\n",
    "            fused_scores[doc] += 1 / (rank + k)\n",
    "            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n",
    "\n",
    "    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "    print(\"Final reranked results:\", reranked_results)\n",
    "    return reranked_results\n",
    "\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fusion_two(original_query, selected_car):\n",
    "    normal_docs = {}\n",
    "    str_docs = {}\n",
    "    folder_path = f'/content/drive/MyDrive/Innovent/manuals/{selected_car}'\n",
    "    for filename in os.listdir(folder_path):\n",
    "      if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        splitted = load_pdf(file_path)\n",
    "        normal_docs[filename] = splitted\n",
    "        str_docs[filename] = str(splitted)\n",
    "    queries = generate_queries(original_query)\n",
    "    all_results = {}\n",
    "    for query in queries:\n",
    "      search_results = vector_search(query, str_docs)\n",
    "      all_results[query] = search_results\n",
    "    reranked_results = reciprocal_rank_fusion_docs(all_results)\n",
    "    required_doc = list(reranked_results.keys())[0] # Takes the top ranked doc from all of them\n",
    "    print(\"SEARCHING IN: \", required_doc)\n",
    "    required_context = normal_docs[required_doc]\n",
    "    db = Chroma.from_documents(required_context, embedding_function)\n",
    "    retriever = db.as_retriever(k=5)\n",
    "    template = \"\"\"You have been provided with the context, the user is asking to troubleshoot, you have to use the context to answer the query\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # langchain.debug = True\n",
    "    chain = (\n",
    "      {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "      | prompt\n",
    "      | gemini\n",
    "      | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(original_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_query(input_text):\n",
    "  prompt = ChatPromptTemplate(\n",
    "  input_variables=['query'],\n",
    "  messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(template='You are a helpful assistant that evaluates the grammatical quality of car troubleshooting queries strictly, returning only a numerical score between 1 and 10.')),\n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(template='Evaluate the grammatical quality of this query: {query}\\nOnly return the score (1-10):'))\n",
    "  ]\n",
    "  )\n",
    "  generate_score = (\n",
    "    prompt | gemini | StrOutputParser()\n",
    "  )\n",
    "  response = generate_score.invoke(input_text)\n",
    "  score = extract_score(response)\n",
    "  print(score)\n",
    "  if((score != None) and (score <=7)):\n",
    "    print(\"Query not good with a score of: \", score)\n",
    "    prompt2 = ChatPromptTemplate(\n",
    "    input_variables=['query'],\n",
    "      messages=[\n",
    "          SystemMessagePromptTemplate(prompt=PromptTemplate(template='You are a helpful assistant that improves car troubleshooting queries by correcting grammar and making them more clear and elaborate.')),\n",
    "          HumanMessagePromptTemplate(prompt=PromptTemplate(template='Improve the grammar and clarity of this query: {query}\\nOutput only the improved query:'))\n",
    "      ]\n",
    "    )\n",
    "    output_query = (\n",
    "      prompt2 | gemini | StrOutputParser()\n",
    "    )\n",
    "    input_text = output_query.invoke(input_text)\n",
    "    print(input_text)\n",
    "\n",
    "  return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722337625.950196 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337625.951041 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1722337626.003197 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337626.004033 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "INFO:     Started server process [1218290]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://peaceful-personally-tadpole.ngrok-free.app\n",
      "Tata Punch\n",
      "how activate wind shield wiper fluid\n",
      "6\n",
      "Query not good with a score of:  6\n",
      "How do I activate the windshield wiper fluid in my car? \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722337660.389722 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337660.390629 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "I0000 00:00:1722337661.257867 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337661.258622 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "I0000 00:00:1722337661.303527 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337661.304169 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "I0000 00:00:1722337661.486623 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337661.487169 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "I0000 00:00:1722337661.933238 1218290 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722337661.934322 1218290 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     103.86.182.226:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [1218290]\n"
     ]
    }
   ],
   "source": [
    "@app.post('/generate')\n",
    "async def generate(prompt_data: dict):\n",
    "    global received_prompt\n",
    "    try:\n",
    "        # Extract the prompt from the incoming JSON payload\n",
    "        # print(\"Hello\")\n",
    "        selected_car = prompt_data['selected_car']\n",
    "        input_text = prompt_data['prompt']\n",
    "        print(selected_car)\n",
    "        print(input_text)\n",
    "        # input_text = prompt_data['prompt']\n",
    "\n",
    "        optimized_query = optimize_query(input_text)\n",
    "        \n",
    "        if not optimized_query:\n",
    "            raise HTTPException(status_code=400, detail=\"Prompt is required\")\n",
    "\n",
    "        # Make the first letter of the optimized query lowercase\n",
    "        optimized_query = optimized_query[0].lower() + optimized_query[1:]\n",
    "        final_prompt = f\"In {selected_car}, {optimized_query}\"\n",
    "        rag = generate_fusion_two(optimized_query, selected_car)\n",
    "        inputs = tokenizer(\n",
    "            [\n",
    "                prompt_template.format(\n",
    "                    final_prompt,  # instruction\n",
    "                    \"\",  # output - leave this blank for generation!\n",
    "                )\n",
    "            ], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
    "        generated_text = tokenizer.batch_decode(outputs)\n",
    "\n",
    "        return {\"generated_text\": generated_text[0], \"input_text\": optimized_query, \"rag\": rag}\n",
    "    except KeyError:\n",
    "        raise HTTPException(status_code=400, detail=\"Prompt not found in request body\")\n",
    "\n",
    "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "auth_token = \"2jomqKcchgXe2FCWs69mV1tAFwG_35XAms6dCCEztbwKiWKmA\"\n",
    "\n",
    "# Set the authtoken\n",
    "ngrok.set_auth_token(auth_token)\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000, domain=\"peaceful-personally-tadpole.ngrok-free.app\")\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
